# -*- coding: utf-8 -*-
"""MobileViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ma438JEEW08CMDjnE1q8RicG-W2y4lUf
"""

import torch
import torch.nn as nn

# Basic Conv block
class ConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size=1, stride=1, padding=0, groups=1, activation=nn.SiLU):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size, stride=stride, padding=padding, groups=groups),
            nn.BatchNorm2d(out_ch),
            activation() if activation is not None else nn.Identity()
        )

    def forward(self, x):
        return self.block(x)

# Inverted Residual block
class ResBlock_Inverted(nn.Module):
    def __init__(self, in_ch, out_ch, stride, expansion=6):
        super().__init__()
        self.residual = (stride == 1 and in_ch == out_ch)
        hidden_ch = in_ch * expansion
        self.block = nn.Sequential(
            ConvBlock(in_ch, hidden_ch, kernel_size=1),
            ConvBlock(hidden_ch, hidden_ch, kernel_size=3, padding=1, groups=hidden_ch),
            ConvBlock(hidden_ch, out_ch, kernel_size=1, activation=None)
        )

    def forward(self, x):
        return x + self.block(x) if self.residual else self.block(x)

# Patch embedding for transformer
class PatchEmbedding(nn.Module):
    def __init__(self, in_ch, embed_dim, patch_size):
        super().__init__()
        self.patch = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.flatten = nn.Flatten(2)

    def forward(self, x):
        x = self.patch(x)
        x = self.flatten(x).transpose(1, 2)
        return x

# Transformer encoder
class Transformer(nn.Module):
    def __init__(self, dim, heads):
        super().__init__()
        self.atten_layer = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)
        self.norm1 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 2),
            nn.GELU(),
            nn.Linear(dim * 2, dim)
        )
        self.norm2 = nn.LayerNorm(dim)

    def forward(self, x):
        atten_out, _ = self.atten_layer(x, x, x)
        x = self.norm1(x + atten_out)
        x = self.norm2(x + self.mlp(x))
        return x

# MobileViT block
class MobileViT_Block(nn.Module):
    def __init__(self, in_ch, trans_dim, patch_size, heads):
        super().__init__()
        self.feature_extr = nn.Sequential(
            ConvBlock(in_ch, in_ch, kernel_size=3, padding=1),
            ConvBlock(in_ch, trans_dim, kernel_size=1)
        )
        self.patch_embedding = PatchEmbedding(trans_dim, trans_dim, patch_size)
        self.transformer = Transformer(dim=trans_dim, heads=heads)
        self.output_proj = nn.Sequential(
            ConvBlock(trans_dim, in_ch, kernel_size=1),
            ConvBlock(in_ch, in_ch, kernel_size=3, padding=1)
        )
        self.patch_size = patch_size

    def forward(self, x):
        y = self.feature_extr(x)
        B, C, H, W = y.shape
        Ph = Pw = self.patch_size
        y_seq = self.patch_embedding(y)
        y_seq = self.transformer(y_seq)
        H_, W_ = H // Ph, W // Pw
        y = y_seq.view(B, H_, W_, -1).permute(0, 3, 1, 2).contiguous()
        return self.output_proj(y)

# Full MobileViT model
class MobileViT(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.stem = nn.Sequential(
            ConvBlock(3, 16, kernel_size=3, stride=2, padding=1),
            ResBlock_Inverted(16, 16, stride=1)
        )
        self.stage2 = ResBlock_Inverted(16, 24, stride=2)
        self.stage3 = MobileViT_Block(24, trans_dim=64, patch_size=2, heads=4)
        self.stage4 = MobileViT_Block(24, trans_dim=80, patch_size=2, heads=4)
        self.stage5 = MobileViT_Block(24, trans_dim=96, patch_size=2, heads=4)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(24, num_classes)

    def forward(self, x):
        x = self.stem(x)
        x = self.stage2(x)
        x = self.stage3(x)
        x = self.stage4(x)
        x = self.stage5(x)
        x = self.pool(x).view(x.size(0), -1)
        return self.classifier(x)