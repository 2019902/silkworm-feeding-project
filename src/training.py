# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ma438JEEW08CMDjnE1q8RicG-W2y4lUf
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

from globals import lr, weight_decay, max_epochs, patience, device, best_model_path
from utils import early_stop

# Standard training loop
def train_model(model, dataloaders):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    train_losses, val_losses, val_accs = [], [], []
    best_val_loss = float('inf')
    epochs_no_improve = 0

    os.makedirs('models', exist_ok=True)

    for epoch in range(1, max_epochs + 1):
        # Training
        model.train()
        running_loss = 0.0
        for imgs, labels in dataloaders['train']:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()
            loss = criterion(model(imgs), labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        train_loss = running_loss / len(dataloaders['train'])
        train_losses.append(train_loss)

        # Validation
        model.eval()
        running_val_loss = 0.0
        correct = total = 0
        with torch.no_grad():
            for imgs, labels in dataloaders['val']:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                running_val_loss += criterion(outputs, labels).item()
                preds = outputs.argmax(dim=1)
                total += labels.size(0)
                correct += (preds == labels).sum().item()
        val_loss = running_val_loss / len(dataloaders['val'])
        val_acc = 100 * correct / total
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        print(f"Epoch {epoch:02d}  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}  Val Acc: {val_acc:.2f}%")

        best_val_loss, epochs_no_improve = early_stop(val_loss, best_val_loss, epochs_no_improve, model, best_model_path)
        if epochs_no_improve >= patience:
            print(f"‚è∏ Early stopping at epoch {epoch}")
            break

    model.load_state_dict(torch.load(best_model_path))
    print("Loaded best model.")

    # Plot curves
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss', marker='o')
    plt.plot(val_losses, label='Val Loss', marker='o')
    plt.title('Loss Curves')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(val_accs, label='Val Acc', marker='o')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend()
    plt.tight_layout()
    plt.show()