# -*- coding: utf-8 -*-
"""Unsupervised_segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S3u8zXeOTm4jcDYnYrgunek91x3ryDOj

**Imports**
"""

import torch
import torch.nn as nn
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from PIL import Image
from torchvision import transforms, models
from torch.utils.data import Dataset, DataLoader, random_split, Subset
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import torch.nn.functional as F
from sklearn.metrics import silhouette_score, adjusted_rand_score

"""**Globals**"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
class ConvBlock(nn.Module):
  def __init__(self, in_ch, out_ch, kernel_size=1, stride=1, padding=0, groups = 1, activation = nn.SiLU):
    super().__init__()
    self.in_ch = in_ch
    self.out_ch = out_ch
    self.stride = stride
    self.activation = activation() if activation is not None else nn.Identity()
    self.block = nn.Sequential(
        nn.Conv2d(in_ch, out_ch, kernel_size, stride = stride, padding = padding, groups = groups),
        nn.BatchNorm2d(out_ch),
        self.activation
    )

  def forward(self, x):
    return self.block(x)


class ResBlock_Inverted(nn.Module):
  def __init__(self, in_ch, out_ch, stride, expansion = 6):
    super().__init__()
    self.in_ch = in_ch
    self.out_ch = out_ch
    self.expansion = expansion
    hidden_ch = in_ch * expansion

    #Expansion
    if stride == 1 and in_ch == out_ch:
      self.residual = True
    else:
      self.residual = False

    self.block = nn.Sequential(
        ConvBlock(in_ch, hidden_ch, kernel_size = 1, padding = 0),
        ConvBlock(hidden_ch, hidden_ch, kernel_size = 3, padding = 1, groups = hidden_ch),
        ConvBlock(hidden_ch, out_ch, kernel_size = 1, padding = 0, activation = None)
    )

  def forward(self, x):
    if self.residual:
      return x + self.block(x)
    else:
      return self.block(x)


class PatchEmbedding(nn.Module):
  def __init__(self, in_ch, embed_dim, patch_size):
    super().__init__()
    self.patch = nn.Conv2d(in_ch, embed_dim, kernel_size = patch_size, stride = patch_size)
    self.flatten = nn.Flatten(2)

  def forward(self, x):
    x = self.patch(x)          # [B, embed_dim, H/ps, W/ps]
    x = self.flatten(x)        # [B, embed_dim, N]
    x = x.transpose(1, 2)      # [B, N, embed_dim]
    return x

class Transformer(nn.Module):
  def __init__(self, dim, heads):
    super().__init__()
    self.atten_layer = nn.MultiheadAttention(embed_dim = dim, num_heads = heads, batch_first = True)
    self.norm1 = nn.LayerNorm(dim)
    self.mlp = nn.Sequential(
        nn.Linear(dim, dim * 2),
        nn.GELU(),
        nn.Linear(dim * 2, dim)
    )
    self.norm2 = nn.LayerNorm(dim)

  def forward(self, x):
    atten_out, _ = self.atten_layer(x, x, x) #atten_layer -> atten_output, atten_weights. I only want atten_out
    x = self.norm1(x + atten_out)
    x = self.norm2(x + self.mlp(x))
    return x

class MobileViT_Block(nn.Module):
  def __init__(self, in_ch, trans_dim, patch_size, heads):
    super().__init__()
    self.feature_extr = nn.Sequential(
        ConvBlock(in_ch, in_ch, kernel_size = 3, padding = 1),
        ConvBlock(in_ch, trans_dim, kernel_size = 1)
    )

    self.patch_embedding = PatchEmbedding(in_ch = trans_dim, embed_dim = trans_dim, patch_size = patch_size)
    self.transformer = Transformer(dim = trans_dim, heads = heads)
    self.output_proj = nn.Sequential(
        ConvBlock(trans_dim, in_ch, kernel_size = 1),
        ConvBlock(in_ch, in_ch, kernel_size = 3, padding = 1)
    )

    self.patch_size = patch_size

  def forward(self, x):
    y = self.feature_extr(x)   # [B, d, H, W], d = trans_dim
    B, C, H, W = y.shape
    Ph = Pw = self.patch_size
    y_seq = self.patch_embedding(y)  # [B, N, dim], N=HxW
    y_seq = self.transformer(y_seq)
    H_, W_ = H // Ph, W // Pw
    y = y_seq.view(B, H_, W_, -1).permute(0, 3, 1, 2).contiguous()
    out = self.output_proj(y)
    return out

class MobileViT(nn.Module):
  def __init__(self, num_classes = 2):
    super().__init__()
    self.stem = nn.Sequential(
        ConvBlock(3, 16, kernel_size = 3, stride = 2, padding = 1),
        ResBlock_Inverted(16, 16, stride = 1)
    )

    self.stage2 = ResBlock_Inverted(16, 24, stride=2)
    self.stage3 = MobileViT_Block(24, trans_dim=64, patch_size=2, heads = 4)
    self.stage4 = MobileViT_Block(24, trans_dim=80, patch_size=2, heads = 4)
    self.stage5 = MobileViT_Block(24, trans_dim=96, patch_size=2, heads = 4)

    self.pool = nn.AdaptiveAvgPool2d(1)
    self.classifier = nn.Linear(24, num_classes)

  def forward(self, x):
    x = self.stem(x)
    x = self.stage2(x)
    x = self.stage3(x)
    x = self.stage4(x)
    x = self.stage5(x)
    x = self.pool(x).view(x.size(0), -1)
    return self.classifier(x)

"""**DATASET CREATION**"""

!pip install --quiet gdown

!gdown "https://drive.google.com/uc?id=1f8QhG08aikAqoPoVrL5v3-nSS_KAFbHB" -O silkworm_dataset.zip
!unzip -q silkworm_dataset.zip -d silkworm_dataset

!ls silkworm_dataset

root = '/content/silkworm_dataset/silk_dataset'
csv_path = os.path.join(root, '0_data.csv')

df = pd.read_csv(csv_path)
print(df.head())

#Trasformazioni
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor()
])

#Dataset custom
class SilkwormDataset(Dataset):
    def __init__(self, df, root_dir, transform=None):
        self.df = df.reset_index(drop=True)
        self.root = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(self.root, row['foto'])
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        label = int(row['classificazione'])  # 0 o 1
        return image, label

#Crea dataset e split
full_ds = SilkwormDataset(df, root, transform)
train_size = int(0.8 * len(full_ds))
val_size   = len(full_ds) - train_size
train_ds, val_ds = random_split(full_ds, [train_size, val_size])

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)

print(f"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}")

"""**UNSUPERVISED CLUSTERING**"""

def evaluate_segmentation_quality(features, labels, method_name):
    try:
        sil_score = silhouette_score(features, labels)
        n_clusters = len(np.unique(labels))
        unique, counts = np.unique(labels, return_counts=True)
        cluster_balance = np.std(counts) / np.mean(counts)

        print(f"\n {method_name} Results:")
        print(f"   Silhouette Score: {sil_score:.4f}")
        print(f"   Number of Clusters: {n_clusters}")
        print(f"   Cluster Balance: {cluster_balance:.4f}")

        return {
            'silhouette_score': sil_score,
            'n_clusters': n_clusters,
            'cluster_balance': cluster_balance
        }
    except Exception as e:
        print(f"Error evaluating {method_name}: {e}")
        return None

def compare_methods(results_dict):
    print("\n" + "="*50)
    print("SEGMENTATION COMPARISON")
    print("="*50)

    best_method = None
    best_score = -1

    for method, results in results_dict.items():
        if results is not None:
            score = results['silhouette_score']
            print(f"{method:15} | Silhouette: {score:.4f} | Clusters: {results['n_clusters']:2d} | Balance: {results['cluster_balance']:.3f}")

            if score > best_score:
                best_score = score
                best_method = method

    print("="*50)
    if best_method:
        print(f"Best Method: {best_method} (Score: {best_score:.4f})")
    print("="*50)

    return best_method

"""300 IMAGES OF THE DATASET"""

dataloader = DataLoader(full_ds, batch_size=8, shuffle=False)
model = MobileViT().to(device)
model.eval()
features_spatial = []

def extract_features(model, x):
    x = model.stem(x)
    x = model.stage2(x)
    return x

with torch.no_grad():
    for images, _ in dataloader:
        images = images.to(device)
        feat = extract_features(model, images)  # [B, C, H, W]
        B, C, H, W = feat.shape
        feat_flat = feat.permute(0, 2, 3, 1).reshape(-1, C)  # [B*H*W, C]
        features_spatial.append(feat_flat.cpu().numpy())

features = np.concatenate(features_spatial, axis=0)
N = 5000
idx_sample = np.random.choice(features.shape[0], size=N, replace=False)
features_sampled = features[idx_sample]

# PCA
pca = PCA(n_components=10)
features_reduced = pca.fit_transform(features_sampled)

# Kmeans
kmeans = KMeans(n_clusters=6, random_state=42)
kmeans_labels_all = kmeans.fit_predict(features_reduced)

# GMM
gmm = GaussianMixture(n_components=6, covariance_type='tied', random_state=42)
gmm_labels_all = gmm.fit_predict(features_reduced)

# Evaluation
kmeans_results = evaluate_segmentation_quality(features_reduced, kmeans_labels_all, "KMeans")
gmm_results = evaluate_segmentation_quality(features_reduced, gmm_labels_all, "GMM+PCA")

# Methods comparison
results = {
    'KMeans': kmeans_results,
    'GMM': gmm_results
}
best = compare_methods(results)
features_reduced_full = pca.transform(features)

kmeans_labels_full = kmeans.predict(features_reduced_full)
gmm_labels_full = gmm.predict(features_reduced_full)

"""IMAGE VISUALIZATION"""

def sobel_filter(imgs):

    device = imgs.device
    kernel_x = torch.tensor([[1, 0, -1],
                             [2, 0, -2],
                             [1, 0, -1]], dtype=torch.float32, device=device).reshape(1, 1, 3, 3)
    kernel_y = torch.tensor([[1, 2, 1],
                             [0, 0, 0],
                             [-1, -2, -1]], dtype=torch.float32, device=device).reshape(1, 1, 3, 3)

    B, C, H, W = imgs.shape
    edges = torch.zeros_like(imgs)

    for c in range(C):
        channel = imgs[:, c:c+1, :, :]
        grad_x = F.conv2d(channel, kernel_x, padding=1)
        grad_y = F.conv2d(channel, kernel_y, padding=1)
        grad = torch.sqrt(grad_x ** 2 + grad_y ** 2)
        edges[:, c:c+1, :, :] = grad

    return edges

indices = [0, 29, 99]
selected_imgs = [full_ds[i][0] for i in indices]
selected_imgs_tensor = torch.stack(selected_imgs).to(device)

def extract_features(model, x):
    x = model.stem(x)
    x = model.stage2(x)
    return x

with torch.no_grad():
    feat = extract_features(model, selected_imgs_tensor)  # [B, C, H, W]

    edges = sobel_filter(feat)  # [B, C, H, W]
    feat_augmented = torch.cat([feat, edges], dim=1)  # [B, 2*C, H, W]

B, C_aug, H, W = feat_augmented.shape
feat_flat = feat_augmented.permute(0, 2, 3, 1).reshape(-1, C_aug).cpu().numpy()

pca = PCA(n_components=10)
features_reduced_local = pca.fit_transform(feat_flat)

# KMeans
kmeans_masks = KMeans(n_clusters=8, random_state=42).fit_predict(features_reduced_local)
kmeans_masks = kmeans_masks.reshape(B, H, W)

# GMM
gmm_masks = GaussianMixture(n_components=8, covariance_type='tied', random_state=42)
gmm_masks = gmm_masks.fit_predict(features_reduced_local)
gmm_masks = gmm_masks.reshape(B, H, W)

# Kmeans evaluation
kmeans_sample_results = evaluate_segmentation_quality(feat_flat, kmeans_masks.flatten(), "KMeans (Samples)")

# GMM evaluation
gmm_sample_results = evaluate_segmentation_quality(features_reduced_local, gmm_masks.flatten(), "GMM+PCA (Samples)")

# Methods comparison
sample_results = {
    'KMeans': kmeans_sample_results,
    'GMM': gmm_sample_results
}
best_sample = compare_methods(sample_results)


print("\n Individual Sample Analysis:")
for i in range(B):
    print(f"\nImage {indices[i]}:")

    # KMeans
    kmeans_unique = len(np.unique(kmeans_masks[i]))
    print(f"  KMeans: {kmeans_unique} segments")

    # GMM
    gmm_unique = len(np.unique(gmm_masks[i]))
    print(f"  GMM:    {gmm_unique} segments")

fig, axes = plt.subplots(B, 3, figsize=(12, 4 * B))

for i in range(B):
    orig = selected_imgs[i].permute(1, 2, 0).cpu().numpy()
    axes[i, 0].imshow(orig)
    axes[i, 0].set_title("Original")
    axes[i, 1].imshow(kmeans_masks[i], cmap='tab10')
    axes[i, 1].set_title("KMeans Segmentation")
    axes[i, 2].imshow(gmm_masks[i], cmap='tab10')
    axes[i, 2].set_title("GMM Segmentation")
    for j in range(3):
        axes[i, j].axis('off')

plt.tight_layout()
plt.show()